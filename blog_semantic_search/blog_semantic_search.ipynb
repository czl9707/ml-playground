{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e733c77",
   "metadata": {},
   "source": [
    "# A Tiny Project Meant to Get Familiar with HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a6b67",
   "metadata": {},
   "source": [
    "- Have a local .env file created, with `CMS_URL` and `ADMIN_APIKEY` defined inside.\n",
    "- Data are blog data are stored as `.data/blogs.jsonl`, along with embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ce2dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will fetch contents from https://zane-n-kiyo-admin.kiyo-n-zane.com\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import Any, Iterable\n",
    "\n",
    "load_dotenv(\".env.local\")\n",
    "CMS_URL = os.environ[\"CMS_URL\"]\n",
    "API_KEY = os.environ[\"ADMIN_APIKEY\"]\n",
    "\n",
    "print(f\"will fetch contents from {CMS_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ea7f23",
   "metadata": {},
   "source": [
    "### Fetch data from remote to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd55826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "def graphql_fetch(query: str, variables: dict[str, Any] = {}) -> dict:\n",
    "    graphql_url = f\"{CMS_URL}/api/graphql\"\n",
    "\n",
    "    response = requests.post(\n",
    "        graphql_url,\n",
    "        data=json.dumps({\n",
    "            \"query\": query,\n",
    "            \"variables\": variables\n",
    "        }),\n",
    "        headers={\n",
    "            'Content-Type': 'application/json',\n",
    "            \"Accept\": 'application/json',\n",
    "            \"Authorization\": f\"users API-Key {API_KEY}\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    response_obj = json.loads(response.content)[\"data\"]\n",
    "    return response_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53bba45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "get_all_blogs_gql = \"\"\"\n",
    "query {\n",
    "    ZaneDevBlogs {\n",
    "        docs {\n",
    "            link\n",
    "        }\n",
    "    }\n",
    "}\"\"\"\n",
    "\n",
    "get_blog_detail_gql = \"\"\"\n",
    "query ZaneDevBlogByLink($link: String!) {\n",
    "    ZaneDevBlogs (\n",
    "        where: {\n",
    "            link: {\n",
    "                equals: $link\n",
    "            }\n",
    "        }\n",
    "    ) {\n",
    "    docs {\n",
    "        title\n",
    "        tags\n",
    "        link\n",
    "        createdDate\n",
    "        description\n",
    "        content\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "DATA_DIR = Path(\".data_dir\")\n",
    "BLOG_DATA_PATH = DATA_DIR.joinpath(\".blogs.jsonl\")\n",
    "\n",
    "\n",
    "def fetch_blogs_content_to_local() -> None:\n",
    "    blog_links: list[str] = [doc[\"link\"] for doc in graphql_fetch(get_all_blogs_gql)[\"ZaneDevBlogs\"][\"docs\"]]\n",
    "    print(blog_links)\n",
    "\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        os.mkdir(DATA_DIR)\n",
    "    if os.path.exists(BLOG_DATA_PATH):\n",
    "        os.remove(BLOG_DATA_PATH)\n",
    "\n",
    "    with open(BLOG_DATA_PATH, 'a') as j_file:\n",
    "        for link in blog_links:\n",
    "            j_file.write(\n",
    "                json.dumps(\n",
    "                    graphql_fetch(get_blog_detail_gql, {\"link\": link})[\"ZaneDevBlogs\"][\"docs\"][0]\n",
    "                )\n",
    "            )\n",
    "            j_file.write(\"\\n\")\n",
    "\n",
    "def read_blogs_from_local() -> Iterable[dict]:\n",
    "    with open(BLOG_DATA_PATH, 'r') as j_file:\n",
    "        while line := j_file.readline():\n",
    "            yield json.loads(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60041ff7",
   "metadata": {},
   "source": [
    "Execute following only when data is not local available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8032eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['back_to_css_after_everything', '5_wtf_moments_in_python', 'decorator_design_pattern', 'cross_component_styling_with_react_compound_pattern', 'from_monolithic_to_react_compound_pattern', 'solutions_to_bring_asynchronism_into_pytest']\n"
     ]
    }
   ],
   "source": [
    "# fetch_blogs_content_to_local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed375597",
   "metadata": {},
   "source": [
    "### Generate Embeddings for each blog\n",
    "\n",
    "Embeddings are stored in a seperate json file, just for ease of this tiny project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6860881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5cd26b4be14378be2e18c23db832fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c817ca421b4d65a4eac246904b389d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a9789f1638461ea7254ef42996e066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f23f53c7f4345b8bf5711585c02e353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de21440600f4380a4ebfa2d8a1cdf23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88de66682f2c43b3901e83dd60f04a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch \n",
    "\n",
    "# checkpoint = \"distilbert-base-uncased\"\n",
    "# model: transformers.DistilBertModel = transformers.DistilBertModel.from_pretrained(checkpoint)\n",
    "# tokenizer: transformers.DistilBertTokenizer = transformers.DistilBertTokenizer.from_pretrained(checkpoint)\n",
    "checkpoint = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model: transformers.AutoModel = transformers.AutoModel.from_pretrained(checkpoint)\n",
    "tokenizer: transformers.AutoTokenizer = transformers.AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_text_with_chunks(text: str, max_chunk_size: int=500, stride: int=50):\n",
    "    all_tokens: torch.Tensor = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")[0]\n",
    "\n",
    "    start_idx = 0\n",
    "    while start_idx < len(all_tokens):\n",
    "        end_idx = start_idx + max_chunk_size\n",
    "        chunk = all_tokens[start_idx: end_idx]\n",
    "        yield chunk.reshape(1, len(chunk))\n",
    "        start_idx = end_idx - stride\n",
    "\n",
    "def generate_embeddings(text: str) -> torch.Tensor:\n",
    "    chunk_embeddings = [\n",
    "        model(input_ids=ids).last_hidden_state.mean(dim=1, keepdim=True) \n",
    "        for ids in tokenize_text_with_chunks(text)\n",
    "    ]\n",
    "    embedding = torch.cat(chunk_embeddings, dim=1).mean(dim=1)\n",
    "\n",
    "    return embedding.reshape(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5afd3c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOG_EMBEDDING_PATH = DATA_DIR.joinpath(\".embeddings.jsonl\")\n",
    "\n",
    "def write_embeddings_for_blogs_to_local():\n",
    "    if os.path.exists(BLOG_EMBEDDING_PATH):\n",
    "        os.remove(BLOG_EMBEDDING_PATH)\n",
    "\n",
    "    with open(BLOG_EMBEDDING_PATH, \"a\") as j_file:\n",
    "        for blog in read_blogs_from_local():\n",
    "            embedding = generate_embeddings(blog[\"content\"])\n",
    "            j_file.write(\n",
    "                json.dumps(\n",
    "                    {\"link\": blog[\"link\"], \"embeddings\": embedding.tolist()}\n",
    "                )\n",
    "            )\n",
    "            j_file.write(\"\\n\")\n",
    "\n",
    "def read_embeddings_from_local():\n",
    "    with open(BLOG_EMBEDDING_PATH, 'r') as j_file:\n",
    "        while line := j_file.readline():\n",
    "            yield json.loads(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbd1525",
   "metadata": {},
   "source": [
    "Execute following only when embeddings is not local available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9b4722a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2379 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# write_embeddings_for_blogs_to_local()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09df2153",
   "metadata": {},
   "source": [
    "### Get a string and find a blog most relevant to!\n",
    "\n",
    "Yes I am excited!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "482e2a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT give me this, I should cycle back to the math.\n",
    "def cosine_similarity_torch(a, b):\n",
    "    a = torch.nn.functional.normalize(a, p=2, dim=-1)\n",
    "    b = torch.nn.functional.normalize(b, p=2, dim=-1)\n",
    "    return torch.matmul(a, b.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c30a21fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_blog(text: str) -> tuple[str, float]:\n",
    "    target_emd = generate_embeddings(text)\n",
    "    largest_cosine = 0\n",
    "    blog_link = \"\"\n",
    "\n",
    "    for blog in read_embeddings_from_local():\n",
    "        blog_emd = torch.Tensor(blog[\"embeddings\"])\n",
    "        cosine = cosine_similarity_torch(blog_emd, target_emd)\n",
    "        if cosine > largest_cosine:\n",
    "            largest_cosine = cosine\n",
    "            blog_link = blog[\"link\"]\n",
    "            \n",
    "    return blog_link, largest_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b893da6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('solutions_to_bring_asynchronism_into_pytest',\n",
       " tensor(0.4952, grad_fn=<DotBackward0>))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"pytest have its plugin to handle asyncio\"\n",
    "find_closest_blog(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
